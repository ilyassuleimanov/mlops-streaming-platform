import json
import time
import uuid
import redis
from kafka import KafkaProducer

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
REDIS_HOST = "localhost" # –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ, —Å–Ω–∞—Ä—É–∂–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
KAFKA_BOOTSTRAP_SERVERS = "localhost:29092" # –í–Ω–µ—à–Ω–∏–π –ø–æ—Ä—Ç Kafka
KAFKA_TOPIC = "inference_queue"

def main():
    print("--- Starting Streaming Verification ---")

    # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis (Feature Store)
    try:
        r_features = redis.Redis(host=REDIS_HOST, port=6379, db=0, decode_responses=True)
        r_features.ping()
        print("‚úÖ Connected to Redis (Feature Store)")
    except Exception as e:
        print(f"‚ùå Failed to connect to Redis: {e}")
        return

    # 2. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis (Predictions Store)
    try:
        r_predictions = redis.Redis(host=REDIS_HOST, port=6379, db=2, decode_responses=True)
        print("‚úÖ Connected to Redis (Predictions Store)")
    except Exception as e:
        print(f"‚ùå Failed to connect to Redis: {e}")
        return

    # 3. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Kafka
    try:
        producer = KafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        print("‚úÖ Connected to Kafka Producer")
    except Exception as e:
        print(f"‚ùå Failed to connect to Kafka: {e}")
        return

    # 4. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    # –ë–µ—Ä–µ–º –ª—é–±–æ–π –∫–ª—é—á –∏–∑ Feature Store
    keys = r_features.keys("*")
    if not keys:
        print("‚ùå Feature Store (Redis db=0) is empty! Run the Airflow pipeline first.")
        return
    
    # –ë–µ—Ä–µ–º feature_key (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–≤—ã–π –ø–æ–ø–∞–≤—à–∏–π—Å—è)
    # –ö–ª—é—á–∏ –≤ load_to_redis.py —Å–æ—Ö—Ä–∞–Ω—è–ª–∏—Å—å –∫–∞–∫ user_id (–ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ)
    feature_key = keys[0]
    print(f"‚ÑπÔ∏è  Using feature key (user_id): {feature_key}")

    # 5. –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Kafka
    request_id = str(uuid.uuid4())
    message = {
        "request_id": request_id,
        "feature_key": feature_key
    }
    
    print(f"üöÄ Sending request to Kafka topic '{KAFKA_TOPIC}'...")
    print(f"   Payload: {json.dumps(message)}")
    
    producer.send(KAFKA_TOPIC, message)
    producer.flush()
    print("‚úÖ Message sent.")

    # 6. –û–∂–∏–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ Redis (db=2)
    result_key = f"prediction_{request_id}"
    print(f"‚è≥ Waiting for result in Redis key: {result_key}...")

    max_retries = 20
    for i in range(max_retries):
        result_json = r_predictions.get(result_key)
        if result_json:
            result = json.loads(result_json)
            print(f"‚úÖ Result found in Redis!")
            print(f"   Prediction: {result.get('prediction')}")
            print(f"   Processed At: {result.get('processed_at')}")
            print("\nüéâ VERIFICATION SUCCESS! The streaming pipeline is working.")
            return
        
        time.sleep(1)
        print(f"   Polling ({i+1}/{max_retries})...")

    print("\n‚ùå VERIFICATION FAILED. Timeout waiting for result.")
    print("Check 'docker compose logs inference' for errors.")

if __name__ == "__main__":
    main()
